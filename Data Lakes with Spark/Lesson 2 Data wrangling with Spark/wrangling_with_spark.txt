-------------------- Wrangling data with Spark --------------------
Spark is written in a functional programming language.. Scala. You can use Spark with other languages such as Java, R, Python

----- Why use functional programming -----
Functional programming is perfect for distributed systems. 


----- MAPS -----
Most common function in Spark.
We use lambda functions a lot in Spark (see jupyter notebook on this)


----- Data formats -----
The most common data formats when using spark will be:
• CSV
• JSON
• HTML
• XML

----- Distributed data stores -----
• S3
• HDFS 




-------------------- Spark Session --------------------
The first component to a spark session is the SparkContext. This is the main entry point and connects the cluster with the application. 

To create a spark context we need to import and set everything up.
E.G 
from pyspark import SoarkContext, SparkConf

configure = SparkConf().setAppName('name').setMaster('IP Address') or ('local') - if its being ran in local mode

sc = SparkContext(conf = configure)

-----------------

To read dataframes we need to import the SQL part of spark
E.G
from pyspark sql import SparkSession 

spark = SparkSession \ 
	.builder \
	.appName('app name')\
	.config('config option', 'config value') \
	.getOrCreate() - this means it will create the session if it hasnt already been done, otherwise it will create a new one.




-------------------- Imerative vs Declarative Programming --------------------
----- Imperative Programming -----
Spark Dataframes & python
- Cares about 'How?' (how to get to the end result)
E.G your sisters birthday
	- get in your car and drive to bakery
	- buy a cake from the bakery
	- drive home and give it to your sister


----- Declarative Programming -----
SQL
- Cares about 'What?' (what the end result is)
E.G, your sisters birthday
	- get a cake




-------------------- Data Wrangling with Dataframes --------------------
----- General Functions -----
Most functions we use with Sparka re similar to Pandas:
• select(): returns a new dataframe with selected columns
• filter(): filters rows using a certain column
• where(): is just an alias for filter()
• groupBy(): groupd the dataframe using the speccified columns, so we can run aggregation on them
• sort(): returns a new dataframe sorted by the specified column(s), By default the second parameter 'ascending' is True
• dropDuplicates(): returns a new dataframe with unique rows based on all or just a subset of columns
• withColumn(): returns a new dataframe by adding a column or replacing the existing one that has the same name. The first parameter is the name of the column, the seccond is an expression of how to compute it.


----- Aggregate Functions -----
Spark SQL had built in mathods for most common aggregations:
• count()
• countDistinct()
• avg()
• max()
• min() 
Need to be careful, Python has built-in methods like min() and max() so you have to be careful not to use them interchangebly.
There are many ways to express the same aggregations. 
- Tocompute one type of aggregation for one or more columns of the DF we can simply chain the aggregate method after groupBy()
- If we want to use different functions on different columns agg() comes in.. E.g. agg({'salary' : 'avg', 'age' : 'max'}) computes the average salarry and maximum age.


----- User Defined Functions (UDF) -----
In Spark we can define our own dunftions with the udf method from the pyspark.sql.funcitons module. The default type of the returned variable for UDFs is a string. If we want to return another type we need to explicitly do so by using the different types from pyspark.sql.functions module.



----- Window Functions -----
Window functions are a way of combining the values of ranges of rows in a DF. When defining the window we can choose how to sort and group (with the partitionBy method) the rows and how wide of a window we'd like to use (described by rangeBetween or rowsBetween).

For more info look at the links below:
- Spark SQL, Dataframes and Datasets guide
https://spark.apache.org/docs/latest/sql-programming-guide.html
- Spark Python API docs
https://spark.apache.org/docs/latest/api/python/index.html




-------------------- Spark SQL --------------------
Some good resources that might be helpful when working with Spark SQL
- Spark SQL built in functions:
	https://spark.apache.org/docs/latest/api/sql/index.html
- Spark SQL guide:
	https://spark.apache.org/docs/latest/sql-getting-started.html



-------------------- RDDs --------------------
----- Resilient Distributed Datasets -----
RDDs are a low-level abstraction of the data. We worked with RDDs in the first Spark version. You can think of RDDs as long lists distributed across various machines. You can still use RDDs as parks of the Spark code although data frames and SQl are easier.

- RDD programming guide:
	https://spark.apache.org/docs/latest/rdd-programming-guide.html

