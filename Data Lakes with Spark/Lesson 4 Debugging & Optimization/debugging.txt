-------------------- Debugging is HARD --------------------
• Previously we ran Spakr in local mode where we can easily fix the code on our laptop because we can view the error on the local machine.
• For Standalone mode, the cluster (group of manage and executor) load data, distributes the task among them and the executor executes the code.The result is either a succesful output or a log of errors, which makes it important to intertret the syntax of logs - this can get tricky.
• One thing that makes the standalone mode difficult to deploy code is that your laptop environments will be completely different than AWS EMR or other cloud systems. As a result you will always have to test your code rigorously on different environment ssettings to make sure the code works.

----- Syntax Errors -----
One main thing to remember is Spark uses 'Lazy Evaluation', so it waits to run code on the data, and it wont error straight away!!!


-----Data Errors -----
You can get data errors come up if any part of the data doesnt match the schema and if NaN or nulls are present. (it wont always crash your code either!!)

----- Debugging your code -----
Remember spark always works with a copy of the input data, so the main data isnt effected. So that means, with the original debugging variable code, it wont get loaded onto the worker nodes.. To get around these limitations Spakr gives us special variables knows as accumulators. Accumulators are like global variables for the entire cluster! 




-------------------- Accumulators --------------------
Accumulators are variables that accumulate. Because Spark runs in distributed mode, the workers are running in parralel, but asynchronously. 
e.g. 
Worker 1 will not be able to know how far worker 2 and worker 3 are done with their tasks. With the same analogy, the variables that are local to workers are not going ot be shared to another worker unless you acumulate them. Accumulators are mostly used for sum operations, like in Hadoop MapReduce, but we can implelement them to do otherwise. 
Accumulators can be useful but they can also be a nightmare, so make sure you use them carefully!!! 

----- Spark Broadcast ----- 
Spark Broadcast variables are secured, read-only variables that get disributed and cached to worker nodes. This is helful to Spark because when the driver sends packets of information to worker nodes, it sends the data and tasked attached together which could be heavier on the netowrk side. Broadcast variable seek to reduce overhead and to reduce communications. Broadcast variables are only used with Spark Context.
- Broadcast join allows you to join large tables to small tables in Spark.  




-------------------- Spark UI --------------------
Spark has a User Interface and it helps measure the health of our Spakr jobs.. 
The Spark UI shows the cluster configuration, DAG broken up into stages and within each stage there are individual tasks
Spark uses a variety of ports to connect to its nodes. Some of the common port we will use from time to time are:
8080 - UI for the master node IMPORTANT 
8888 - Jupyter notebooks
4040 - Active Spark jobs




-------------------- Transformations and Actions --------------------
There are two types of functions in Spark
1. Transformations
2. Actions

Spark uses lazy evalutaions to evaluate RDD and datatframe. This means the code is not executed until it is needed. The actions trigger the lasily evaluated functions. 
e.g.
df - spark.read.load('some csv file')
df1 = df.select('some column').filter('some condition')
df1.write('to path')

• In the code above, 'select' and 'filter' are tranformation functions and 'write' is an action function
• If you execute this code line by line, the second line will be loaded, but we will not see the function being executed in the Spark UI
• When we actually execute using the action 'write' then we will see our Spark program being executed:
	• select --> filter --> write chained in Spark UI
	• but we will only see write show up under our tasks
This is significant becasue we can chain our RDD or dataframes as much as we want, but it mught not do anything until we acutally trigger with some action words. And if we have lengthy transformations, then it might take the executors quite some time to complete tasks.

For further reading on Spark UI use:
https://spark.apache.org/docs/latest/monitoring.html




-------------------- Data Skewness --------------------
In the real world there will be a lot of cases where data is skewed. Skewed data meanss due to non-optimal partitioning, the data is heavy on few partitions (see pictures). In other word if the data isnt partitioned well, you will have work loads waiting on other work loads to finish and when you you are getting billed by the time the data is being processed, you want it to be as quick as possible.

In the jupyter notebook test-skewness you can see most of the data is in 2014-2015. We want to process the dataset through spark using different partitions, if possible. Some ways to solve skewness:
• Data preprocess
• Broadcast joins
• Salting

----- How do we solve skewed data problems? -----
The goal is to change the partitioning to take out the data skewness (e.g. the year column is skewed)... 

1. Use alternative columnas that are more normally distributed: 
	Instead of the 'year' column we could use the 'Issue_Date' column that isnt skewed

2. Make composite keys:
	We can make composite keys by combining two columns so that the new column can be used as a composite key. For example combining 'Issue_Date' and 'State' columns to make a new composite key 'Issue_Date + State'. The new column will now include the data from 2 columns e.g '2017-04-15-NY'. This column can be used to partition the data, create more normally distributed datasets (e.g. distribution of parking violations on 2017-04-15 would now be more spread out across states, and this can now help address skewness)

3. Partition by number of Spark workers:
	Another easy way is using Spark workers. If you know the number of workers for Spark, then you can easily partition the data by number of workers 'df.repartition(number_of_workers)' to repartition your data evenly across your workers. For example, if you have 8 workers, then you should do 'df.repartition(8)' before doing any operations.












