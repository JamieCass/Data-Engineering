-------------------- Debugging is HARD --------------------
• Previously we ran Spakr in local mode where we can easily fix the code on our laptop because we can view the error on the local machine.
• For Standalone mode, the cluster (group of manage and executor) load data, distributes the task among them and the executor executes the code.The result is either a succesful output or a log of errors, which makes it important to intertret the syntax of logs - this can get tricky.
• One thing that makes the standalone mode difficult to deploy code is that your laptop environments will be completely different than AWS EMR or other cloud systems. As a result you will always have to test your code rigorously on different environment ssettings to make sure the code works.

----- Syntax Errors -----
One main thing to remember is Spark uses 'Lazy Evaluation', so it waits to run code on the data, and it wont error straight away!!!


-----Data Errors -----
You can get data errors come up if any part of the data doesnt match the schema and if NaN or nulls are present. (it wont always crash your code either!!)

----- Debugging your code -----
Remember spark always works with a copy of the input data, so the main data isnt effected. So that means, with the original debugging variable code, it wont get loaded onto the worker nodes.. To get around these limitations Spakr gives us special variables knows as accumulators. Accumulators are like global variables for the entire cluster! 




-------------------- Accumulators --------------------
Accumulators are variables that accumulate. Because Spark runs in distributed mode, the workers are running in parralel, but asynchronously. 
e.g. 
Worker 1 will not be able to know how far worker 2 and worker 3 are done with their tasks. With the same analogy, the variables that are local to workers are not going ot be shared to another worker unless you acumulate them. Accumulators are mostly used for sum operations, like in Hadoop MapReduce, but we can implelement them to do otherwise. 
Accumulators can be useful but they can also be a nightmare, so make sure you use them carefully!!! 

----- Spark Broadcast ----- 
Spark Broadcast variables are secured, read-only variables that get disributed and cached to worker nodes. This is helful to Spark because when the driver sends packets of information to worker nodes, it sends the data and tasked attached together which could be heavier on the netowrk side. Broadcast variable seek to reduce overhead and to reduce communications. Broadcast variables are only used with Spark Context.
- Broadcast join allows you to join large tables to small tables in Spark.  




-------------------- Spark UI --------------------
Spark has a User Interface and it helps measure the health of our Spakr jobs.. 
The Spark UI shows the cluster configuration, DAG broken up into stages and within each stage there are individual tasks
Spark uses a variety of ports to connect to its nodes. Some of the common port we will use from time to time are:
8080 - UI for the master node IMPORTANT 
8888 - Jupyter notebooks
4040 - Active Spark jobs




-------------------- Transformations and Actions --------------------
There are two types of functions in Spark
1. Transformations
2. Actions

Spark uses lazy evalutaions to evaluate RDD and datatframe. This means the code is not executed until it is needed. The actions trigger the lasily evaluated functions. 
e.g.
df - spark.read.load('some csv file')
df1 = df.select('some column').filter('some condition')
df1.write('to path')

• In the code above, 'select' and 'filter' are tranformation functions and 'write' is an action function
• If you execute this code line by line, the second line will be loaded, but we will not see the function being executed in the Spark UI
• When we actually execute using the action 'write' then we will see our Spark program being executed:
	• select --> filter --> write chained in Spark UI
	• but we will only see write show up under our tasks
This is significant becasue we can chain our RDD or dataframes as much as we want, but it mught not do anything until we acutally trigger with some action words. And if we have lengthy transformations, then it might take the executors quite some time to complete tasks.

For further reading on Spark UI use:
https://spark.apache.org/docs/latest/monitoring.html




-------------------- Data Skewness --------------------













