-------------------- Debugging is HARD --------------------
• Previously we ran Spakr in local mode where we can easily fix the code on our laptop because we can view the error on the local machine.
• For Standalone mode, the cluster (group of manage and executor) load data, distributes the task among them and the executor executes the code.The result is either a succesful output or a log of errors, which makes it important to intertret the syntax of logs - this can get tricky.
• One thing that makes the standalone mode difficult to deploy code is that your laptop environments will be completely different than AWS EMR or other cloud systems. As a result you will always have to test your code rigorously on different environment ssettings to make sure the code works.

----- Syntax Errors -----
One main thing to remember is Spark uses 'Lazy Evaluation', so it waits to run code on the data, and it wont error straight away!!!


-----Data Errors -----
You can get data errors come up if any part of the data doesnt match the schema and if NaN or nulls are present. (it wont always crash your code either!!)

----- Debugging your code -----
Remember spark always works with a copy of the input data, so the main data isnt effected. So that means, with the original debugging variable code, it wont get loaded onto the worker nodes.. To get around these limitations Spakr gives us special variables knows as accumulators. Accumulators are like global variables for the entire cluster! 




-------------------- Accumulators --------------------
Accumulators are variables that accumulate. Because Spark runs in distributed mode, the workers are running in parralel, but asynchronously. 
e.g. 
Worker 1 will not be able to know how far worker 2 and worker 3 are done with their tasks. With the same analogy, the variables that are local to workers are not going ot be shared to another worker unless you acumulate them. Accumulators are mostly used for sum operations, like in Hadoop MapReduce, but we can implelement them to do otherwise. 
Accumulators can be useful but they can also be a nightmare, so make sure you use them carefully!!! 

----- Spark Broadcast ----- 
Spark Broadcast variables are secured, read-only variables that get disributed and cached to worker nodes. This is helful to Spark because when the driver sends packets of information to worker nodes, it sends the data and tasked attached together which could be heavier on the netowrk side. Broadcast variable seek to reduce overhead and to reduce communications. Broadcast variables are only used with Spark Context.
- Broadcast join allows you to join large tables to small tables in Spark.  




-------------------- Spark UI --------------------
Spark has a User Interface and it helps measure the health of our Spakr jobs.. 
The Spark UI shows the cluster configuration, DAG broken up into stages and within each stage there are individual tasks
Spark uses a variety of ports to connect to its nodes. Some of the common port we will use from time to time are:
8080 - UI for the master node IMPORTANT 
8888 - Jupyter notebooks
4040 - Active Spark jobs




-------------------- Transformations and Actions --------------------
There are two types of functions in Spark
1. Transformations
2. Actions

Spark uses lazy evalutaions to evaluate RDD and datatframe. This means the code is not executed until it is needed. The actions trigger the lasily evaluated functions. 
e.g.
df - spark.read.load('some csv file')
df1 = df.select('some column').filter('some condition')
df1.write('to path')

• In the code above, 'select' and 'filter' are tranformation functions and 'write' is an action function
• If you execute this code line by line, the second line will be loaded, but we will not see the function being executed in the Spark UI
• When we actually execute using the action 'write' then we will see our Spark program being executed:
	• select --> filter --> write chained in Spark UI
	• but we will only see write show up under our tasks
This is significant becasue we can chain our RDD or dataframes as much as we want, but it mught not do anything until we acutally trigger with some action words. And if we have lengthy transformations, then it might take the executors quite some time to complete tasks.

For further reading on Spark UI use:
https://spark.apache.org/docs/latest/monitoring.html




-------------------- Data Skewness --------------------
In the real world there will be a lot of cases where data is skewed. Skewed data meanss due to non-optimal partitioning, the data is heavy on few partitions (see pictures). In other word if the data isnt partitioned well, you will have work loads waiting on other work loads to finish and when you you are getting billed by the time the data is being processed, you want it to be as quick as possible.

In the jupyter notebook test-skewness you can see most of the data is in 2014-2015. We want to process the dataset through spark using different partitions, if possible. Some ways to solve skewness:
• Data preprocess
• Broadcast joins
• Salting

----- How do we solve skewed data problems? -----
The goal is to change the partitioning to take out the data skewness (e.g. the year column is skewed)... 

1. Use alternative columns that are more normally distributed: 
	Instead of the 'year' column we could use the 'Issue_Date' column that isnt skewed

2. Make composite keys:
	We can make composite keys by combining two columns so that the new column can be used as a composite key. For example combining 'Issue_Date' and 'State' columns to make a new composite key 'Issue_Date + State'. The new column will now include the data from 2 columns e.g '2017-04-15-NY'. This column can be used to partition the data, create more normally distributed datasets (e.g. distribution of parking violations on 2017-04-15 would now be more spread out across states, and this can now help address skewness)

3. Partition by number of Spark workers:
	Another easy way is using Spark workers. If you know the number of workers for Spark, then you can easily partition the data by number of workers 'df.repartition(number_of_workers)' to repartition your data evenly across your workers. For example, if you have 8 workers, then you should do 'df.repartition(8)' before doing any operations.




-------------------- Troubleshooting other Spark issues --------------------
----- Insufficient resources -----
Sometimes there are possible ways of improvement, processing large datasets just takes a longer time than smaller ones even without any big problem in the code or job tuning. Using more resources, either by increasing the number of exectuors or using more power machines, might not be possible. When you have a slow job its useful to understand: 

How much data youre actualy processing (compressed file formats can be tricky to inturpret). If you can decrease the amount of data being processed by filtering or aggregating to lower cardinality, and if resource untilization is reasonable.

There are many cases where different stages of a Spark job differ greatly in their resource needs: loading data is typically I/O heavi, some stages might require a lot of memory, others might need a lot of CPY. Undertsanding these differences might help optomize the overall performance. Use the Spark UI and logs to collect information on these metrics.

If you run into out of memory errors you might consider increasing the number or partitions. If the memory errors occur over time you can look into why the size of cartain objects is increasing too much suring the rin and if the size can be contained. Also look for ways of freeing up resources if garbage collection metrics are high.

Certain algorithms (especially ML) use the driver to store data the workers share and update during the run. If you see memory issues on the driver check if the algorithm youre using is pushing too much data there.


----- Data Skew -----
If you drill down in the Spark UI to the task level you can see if certain partitions process signicantly more data than others and if they are lagging behind. Such symptoms usually indicate a skewed data set. Consider implementing the teqhniques mentioned in this lesson:

Add an intermediate data processing step with an alternative key Adjust the spark.sql.shuffle.patritions parameter if necessary.

The problem with data skew is that its very specific to a dataset. You might know ahead of time that certain customers or accounts are expected to generate a lot more activity but the solution for dealing with skew might be strongly dependent on how the data looks. If you need to implement a more general solution (for example for an automated pipeline) its recommended to take a more concervative approach (so assume that your data will be skewed) and monitor how bad the skew really is.

----- Inefficient queries -----
Once your Spark application works its worth spending some times to analyze the query it runs. You can use the Spark UI to check the DAG and the jobs and stages its built of.

Sparks query optimizer is called Catalyst. While Catalyst is a powerful tool to turn python code to an optimized query plac that can run on the JVM, it has some limitations when optimizing your code. It will for example push filters in a particular stage as early as possible in the plan but wont move a filter across stages. Its your job to make sure that if early filtering is possible without compromising the business logic then you perform this where its more appropriate.

It also cant decide for you how much date youre shuffling across a cluster. Remember from the first lesson how expensive sedning data through the network is. As much as possible try to avoid shuffling unnecessary data. In practice, this means that you need to perform joins and grouped aggregations as late as possible.

When it comes to joins there is more than one strategy to choose from. If one of your dataframes is small consider using broadcast hash join instead of a hash join






