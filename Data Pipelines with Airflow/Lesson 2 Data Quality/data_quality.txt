-------------------- Data Quality -------------------- 
Objective 
• Learn the concepts of data quality and how to apply them to the data pipelines in Airflow




-------------------- Data Lineage --------------------
The data lineage of a datasets describes the discrete steps involved in the creation, movement and calculation of that dataset. 

- Why is data lineage important?

1. Instilling Confidence: Being able to describe the data lineage of a particular dataset or analysis will build confidence in data consumers (engineers, analysts, data scientists, etc..) that our data pipeline is creating meaningful results using the correct datasets. If the data lineage is unclear, its less likely that the data consumers will trust or use the data.

2. Defining Metrics: Another major benefit or surfacing data lineage is that it allows everyone in the organization to agree on the definition of how a particular metric is calculated.

3. Debugging: Data lineage helps data engineers track down root errors when they occur. If each step of the data movements and transformation process is well described, its easy to find problems when they occur.

In general, data lineage has important implications for a business. Each department or business unit's success is tied to data and to the flow of data between departments. for e.g. sales departments rely on data to make sales forcasts, while at the same time the finance department would need to track sales and revenue. Each of these dapartments and roles depend on data, and knowing where to find the data. Data flow and data lineage tools enable data engineers and architects to track the flow of this large web of data. 




 -------------------- Data Pipeline Schedules --------------------
 Pipelines are often driven by schedules which determine what data should be analyzed and when.

 - Why schedules?
	• Pipeline schedules can reduce the amount of data that needs to be processed in a given run. It helps scope the job to only run the data for the time period since the data pipeline last run. In a naive analysis, with no scope we would analyze all of the data at all times.

	• Using schedules to select only data relevant to the time period of the given pipeline execution can help imporve the quality and accuracy of the analyses performed by our pipeline.

	• Running pipelines on a schedule will decrease the time it takes the pipeline to run.

	• An analysis of larger scope can leverage already-completed work. For e.g., if the aggregates for all months prior to now have already been done by a schedule job, then we only need to perfom the aggregation for the current month and add it to the existing totals.


 - Selecting the time period
 Determining the appropriate time period for a schedule is based on a number of factors which you need to consider as the pipeline designer.

1. What is the size of data, on average, for a time period? 
 	If an entire years worth of data is only a few kb or mb, then perhaps its fine to load the entire dataset. If an hours worth of data is hundreds of mb or even in the gbs then likely you will need to schedule your pipeline more frequently.

2. How frequently is data arriving, and how often does the analysis need to be performed? 
	If our bikeshare company needs trip data every hour, that will be a driving factor in determining the schedule. Alternatively, if we have to load hundreds of thousands of tiny records, even if they don't add up to much in terms of mb or gb, the file access alone will slow down our analysis and we’ll likely want to run it more often.

3. What's the frequency on related datasets?
	A good rule of thumb is that the frequency of a pipeline’s schedule should be determined by the dataset in our pipeline which requires the most frequent analysis. This isn’t universally the case, but it's a good starting assumption. For example, if our trips data is updating every hour, but our bikeshare station table only updates once a quarter, we’ll probably want to run our trip analysis every hour, and not once a quarter.




-------------------- Schedules in Airflow -------------------- 
- Start date
Airflow will begin running pipelines on the start date selected. Whenever the start date of a DAG is in the past, and the time different between the start date and now includes more than one schedule instals. Airflow will automatically schedule and execute a DAG run to satisfy each one of those intervals. This feature is useful in almost all enterprise settings, where companies have established years of data that may need to be retroactively analyzed.

- End date 
Airflow pipelines can also have end dates. You can use and end_date with your pipeline to let Airflow know when to stop running the pipeline. End_dats can also be useful when you want to perform an overhaul or redesign of an existing pipeline. Update the old pipeline with an end_date and then have the new pipeline start on the end date of the old pipeline.




-------------------- Data Partitioning --------------------
- Schedule Partitioning
Not only are schedules great for reducing the amount of data our pipelines have to process, but they also help up guarantee that we can meet the timing guarantees that our data consumers may need. 


- Logical Partitioning
Conceptually related data can be partitioned into discrete segments and processed seperately. This process of seperating data based on its conceptual relationship is called logical partitioning. With logical partitioning, unrealated things belong to seperate steps. Consider your dependencies and seperate processing around these boundaries.

Also worth mentioning, the data location is another form of logical partitioning. For example, if our data is stored in a key-value store like Amazon S3 in a format such as 's3://<bucket>/<year>/<month>/<day>' we could say that our date is logically partitioned by time.


- Size Partitioning
Size partitioning seperates data fro processing based on desired or required storage limits. This essentially sets the amount of data included in a data pipeline run. Size partitioning is critical to understand when working with large dataets, especially with Airflow! 


- Why Data Partitioning? 
Pipelines designed to work with partitioned data fail more gracefully. Smaller datasets, smaller time periods and related concepts are easier to debug than big datasets, large time periods and unrelated concepts. Patitioning makes debugging and rerunning failed tasks much simpler. It also enables easier redos of work, reducing cost and time. 

Another great thing about Airflow is that if your data is partitioned appropriately, your tassks will naturally have fewer dependencies on each other. Because of this, Airflow will be able to parrellelize execution of your DAGs to produce your results even faster. 




-------------------- Data Quality -------------------- 
Data quality is the measure of how well a dataset satisfies its intended use. 
Adherence to a set of requirements is a good starting point for measuring data quality.
Requirements should be defined by you and your data consumers before you start creating your data pipeline. 
 
Examples of Data Quality Requirements:
• Data must be a certain size
• Data must be accurate to some margin of error
• Data must arrive within a given timeframe from the start of execution
• Pipelines must run on a particular schedule
• Data must not contain any sensitive information

You would use a Service Level Agreement (SLA) to tell Airflow when a DAG must be completed by.




-------------------- Task Boundaries --------------------
DAG tasks should be designed such that they are:
• Atomic and have a single purpose
• Maximize parallelism
• Make failure stats obvious

Every tak in your DAG should perform only one job 
"Write progrrams that do one thing and do it well"

Benefits of task boundaries
• Re-visitable: Task boundaries are used for you if you revisit a pipeline you wrote after a 6 month absence. Youll have a mush easier time understanding how it works and the lineage of the data if the boundaries between tasks are clear and well defined. This is tru in the code itself, and within the Airflow UI.
• Tasks that do just one thing are often more easily parralelized. This parallelization can offer a significant speedup in the execution of our DAGs. 




-------------------- SubDAGs --------------------
Commonly repeated seris of tasks within DAGs can be captured as reusable SubDAGs. Benefits inclue:
• Decrease the amount of code we need to write and maintain to create a new DAG
• Easier to understand the high level goals of a DAG
• Bug fixes, speedups, and other enhancements can be made more quickly and distributed to all DAGs that us that SubDAG

Drawbacks of using SubDAGs 
• Limit the visibility within the Airflow UI
• Abstraction makes understanding what the DAG is doing more difficult
• Encourages premature optimization




-------------------- Monitoring -------------------- 
Airflow can surface metrics and emails to help you stay on top of pipeline issues. 

- SLAs
Airflowm DAGs  may optionsally specify an SLA or 'Service Level Agreement', which is defined as a time by which DAG is comlete. For time-sensitive applications these features are critical for developing trust amongst your pipeline customers and ensuring that data is delivered while it is still meaningful. Slipping SLAs can also be early indicators of performance problems, or need to scale up the size of your Airflow cluster. 

- Emails and Alerts
Airflow can be configured to send emails on DAG and task state changes. These state changes may include successes, failures or retries. Failure emails can allow you to easily trigger alerts. It is common for alerting systems like PagerDuty to accept emails as a source of alerts. If a mission-critical data pipeline fails, you will need to know as soon as possible to get online and get it fixed. 

- Metrics
Airflow comes out of the box with the ability to send system metrics using a metrics aggregator called statsd. Statsd can be coupled with metrics visualization tools like Grafana to provide you and your team high level insights into the overall performance of your DAGs, jobs and tasks. These systems can be intergrated into your alerting system, such as pagerduty, so that you can ensure problems are dealt with immediately. These Aiflow system-level metrics allow you and your team to stay ahead of issues before the even occur by watching long-term trends.













