-------------------- IMPLEMENTING A DWH ON AWS --------------------
-- Choices for implementing a Data warehouse:
1. Cloud:
   	• Lower barrier to entry
   	• May add as you need - its ok to change your opinion
   	• Scalability & elasticity out of the box


2. On-Premise:
   Thik about:
   	• Heterogeneity, scalability, elasticity of the tools, technologies and processes.
   	• Need for diversse IT staff skills & multiple locatioons
   	• Cost of ownership


----- DWH Dimensional model storage on AWS -----
-- Cloud-Managed:
	• Amazon RDS (Relational Data Storage)
	• Amazon DynamoDB
	• Amazon S3
	Re-use of expertise; Less IT staff for security, upgrades, etc.. Less OpEx
	Deal with complexity with techniques like: 'Infrastructure as code'

-- Self-Managed:
	• EC2 + PostgreSQL
	• EC2 + Cassandra
	• EC2 + Unix FS
	Always 'catch-all' option if needed


														- AWS RDS
												SQL --->- Redshift
		     -> Cloud 		-> Cloud-managed -> NoSQL	  SQL based
		    |									Files     Columnar
DWH Storage |											  Massively parralel
			|
			 -> On-Premise 	-> Self-Managed  -> EC2 + ?




-------------------- Amazon Redshift Technology --------------------
• Column-oriented storage
• Best suited for storing OLAP workloads, summing over a long history
• Internally, its a modified PostgreSQL 

----- Most relational DB -----
- Most relational databases execute multiple queries in parallel if they have access to many cores/servers
- However, every query is always executed on a single CPU of a single machine
- Acceptable for OLTP mostly updates and few rows retrieval

----- MPP -----
- Massively Parallel Processing (MPP) databses parallelizee the execution of one query on multiple CPU's/machines
- How? A table is pertitioned and pertitions are processed in parallel
- Redshift is a cloud managed, column-oriented, MPP database
- Other examples include Teradata, Aster, Oracle Exadata & Azure SQL




-------------------- Amazon Redshift Architecture --------------------
• Client Applications
		|
		v
• Leader Node
	- Coordinates compute nodes
	- HAndles external communication
	- Optimizes query execution
		|
		v
• Compute node 1   -   Compute node n 
	- Each with its own CPU, memory and disk
	- Scale up: get more powerful nodes
	- Scale out: get more nodes
		|
		v
• Node slices
	- Each compute node is logically divided into a number of slices
	- A cluster with n slices can process n partitions of tables simultaneously
		|
		v
• Datawaehouse cluster




-------------------- SQL to SQL ETL --------------------
How would you insert from one table into another table on a different server?

General Solution (see pictures in folder)
----------------
DB Server 1 (e.g. mysql)
(An ETL server can talk to teh source server and run a SELECT query on the source DB server)
			|
		  SELECT
		 	|
			v
		ETL Server
(Stotres the results in CSV files.. Needs large storage)
			| 
		INSERT/COPY
			|
			v
DB Server 2 (e.g. postresql)
(INSERT/COPY the resukts in the destination DB server)



-------------------- SQL to SQL ETL (AWS Case) --------------------
For the AWS example, we store all the data in S3 buckets and just use the EC2 machine to act as a client to RDS and Redshift to issue COPY commands, (we dont need storage on the EC3 machine).

AWS Solution (see pictures in folder)
------------
ETL server(EC2) To talk to AWS RDS instance
	|
	v
AWS RDS
(Copy CSV to S3 Bucket)
	|
	v
ETL server(EC2) to talk to Redshift instance
	|
	v
Redshift
(Copy CSV from S3 Bucket)

S3 offers a very reliable, scalable and worry-free storage solution, but it doesnt offer processing power.




-------------------- Redshift & ETL in Context --------------------
See picture in folder for better understanding
------------------
- Sources -
• Amazon S:
	• Amazon S3
• AWS RD:
	• AWS RDS
• Cassandra
• Amazon DynamoDB
• Amazon EC2
------------------

----- ETL ------
• Amazon EC2

------ Staging -----
• CSV files
• Amazon S3

----- DWH -----
• Amazon Redshift

----- OLAP Cubes -----
• Amazon S3
• Cassandra
• Amazon RDS

----- BI Apps -----
• Jupyter
• Amazon Quicksight




-------------------- Ingesting at Scale --------------------
• To transfer data from a S3 staging area to redshift use the COPY command
• Inserting data row by row using INSERT will be very slow!!!!!
• If the file is large:
	- It is better to break it up into myltiple files
	- Ingest in parralel:
		* Either using a comon prefix
		* Or a manifest file
• Other considerations:
	- Better to ingest from the same AWS region
	- Better to compress all the CSV files
• One can also specify the delimeter to be used




-------------------- Redshift ETL Example --------------------
----- Common prefix example (dont have to specify each file) -----

- Syntax to copy into a table from a s3 bucket: 
COPY sporting_even_ticker 
FROM 's3://udacity-labs/tickets/split/part'
CREDENTIALS 'asw_iam_role=arn:aws:iam::464956546:role/dwhRole'
gzip DELIMITER ';' REGION 'us-west-2';

- What the files look like that you want to copy into your table:
s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00000.csv.gz')
s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00001.csv.gz')
s3.ObjectSummary(bucket_name='udacity-labs', key='tickets/split/part-00002.csv.gz')
etc.... 
no need to specify the names of the files explicitly because its prefixed


----- Manifest file example (need to specify each file) -----

- Syntax to copy into a table from a s3 bucket:
COPY customer
FROM 's3://mybucket/cust.manifest'
IAM_ROLE 'arn:aws:iam::0123456789012:role/MyRedshiftRole'
manifest;

- What the files look like:
{
	'entries': [
		{'url':'s3://mybucket-alpha/2013-10-04-custdata', 'mandatory':true}
		{'url':'s3://mybucket-alpha/2013-10-05-custdata', 'mandatory':true}
		{'url':'s3://mybucket-beta/2013-10-04-custdata', 'mandatory':true}
		{'url':'s3://mybucket-beta/2013-10-05-custdata', 'mandatory':true}
	
	]
}


----- PREFIX file e.g.: (common prefix) -----
s3://mySource/sales-day1.csv.gz
s3://mySource/sales-day2.csv.gz
prefix part = 'sales-day1', 'sales-day2', etc... sales is first

----- MANIFEST file e.g.: (common suffix) -----
s3://mySource/day1-sales.csv.gz
s3://mySource/day2-sales.csv.gz
suffix part = 'day1-sales','day2-sales', etc... sales is after each day




-------------------- Redshift ETL Example (continued) --------------------
----- Redshift ETL automatic compression optimization -----
• The optimal compression strategy for each column type is different
• Redshift give the user control over the compression of each column
• The COPY command makes automatic best-effort compression decisions for each column

----- ETL from other sources -----
• It is also possible to ingest directly using ssh from EC2 machines
• Other than that:
	- S3 needs to be used as a staging area
	- Usually an EC3 ETL worker needs to run the ingestion jobs orchestrated by a dataflow product like Airflow, Luigi, Nifi, StreamSet or AWS Data Pipeline


----- ETL out of Redshift -----
• Redshift is accessible, like any relational database, as a JDCB/ODBC source
	- Naturally used by BI apps
• However, we may need to extract data out of Redshift to pre-aggregated OLAP cubes
- Syntax:
UNLOAD ('SELECT * FROM venue LIMIT 10')
to 's3://mybucket/venue_pipe_'
iam_role 'arn:aws:iam::0123456789012:role/MyRedshiftRole'




-------------------- Infrastructure as Code on AWS --------------------
----- Configuring Redshift for S3 and external access -----
• Naturally we can accomplish our goal by going through lots of screenshots/videos or click and fill instructions.
• That said, we take this as an opportunity to introduce an important technique for modern data engineers, namely: Infrastructure-as-Code (IaC).
• An advantage of being in the cloud is the ability to create infrastructure, i.e. machines, users roles, folders and processes using code.
• IaC lets you automate, maintain, deploy, replicate and share complex infrastructures as easily as you maintain code (undreamt-of in an on-premise deployment) e.g. 'Creating a machine as easy as opening a file'.
• IaC is borderline dataEng/devOps


----- IaC options -----
• We have a number of options to acheive Iac on AWS
	- aws-cli scripts (command-line)
	- AWS SDK
	- Amazon Cloud formation

	--------------------------------------

	--- aws cli scripts ---
	• Similar to bach scripts
	• Simple and convenient

	--- AWS SDKs ---
	• Available in lots of programming languages
	• More powerful, could be integrated with apps

	--- Cloud formation ---
	• Json description of all resources, permission, contraints
	• Atomic, either all succeed or all fail




-------------------- Our IaC choice  --------------------
• We will use the python AWS SDK aka boto3
• We will create one IAM user called dwhadmin
• We will give admin privileges
• We use its access token and secret to build our cluster and configure it, that should be our last 'click and fill' process




-------------------- Optimizing table design --------------------
• When a table is partitioned up into many pieces and distributed accross slices in different machines, this is done blindly
• If one has an idea about the frequent access pattern of a tbale, one can choose a more clever strategy
• The 2 possible strategies are:
	- Distribution style
	- Sorting key


----- Distribution styles -----
• EVEN distribution
• ALL distribution
• AUTO distribution
• KEY distribution


	--- EVEN --- (see picture for example)
	• Round-robin over all slices to achieve load-balancing
	• Good if a table wont be joined
	Even distribution is not cost effective and will result in slow performance.

	--- ALL --- (see picture for example)
	• Distributes a table on all slices.
	• A lot of replications but easier to join (no shuffeling)
	• Used frequently for dimensional tables (AKA boradcasting)
	All distribution is more cost effective and easier to JOIN.

	--- AUTO --- (see picture for example)
	• Leaves distribution to Redshift
	• 'Small enough' tables are distributed with an ALL strategy
	• Large tables are distributed with EVEN strategy
	
	--- KEY --- (see picture for example)
	• Rows having similar values are places in the same slice
	• This can lead to skewed distribution if some values of the dist key are more frequent than others
	• Very useful when dimension table is too big to be distributed with ALL strategy. In that case we distribute both the fact table and dimension using the same key
	• If two tables are distributed on the joing key, Redshift collocates the rows from both tables on the same slice




-------------------- Sorting Key -------------------- 
• One can define its columns as a sort key
• Upon loading, rows are sorted before distribution to slices
• Minimizes the quiery time since each node already has contiguous ranges of rows based on the sorting key
• Useful for columns that are frequently in sorting like the date dimensions and its corresponding foreign key in the fact table












